{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# `LlamaCpp` examples\n",
    "\n",
    "This notebook contains usage examples of the llama-cpp-python support in guidance.\n",
    "The first example is RPG character json generator.\n",
    "The second example takes a jsonformer scheme as input and generate a valid json.\n",
    "The third example uses the chat mode of guidance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-24T20:46:56.433152200Z",
     "start_time": "2023-05-24T20:46:55.570187500Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nielswarncke/miniconda3/envs/textgen/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "llama_model_loader: loaded meta data with 19 key-value pairs and 291 tensors from /Users/nielswarncke/models/gguf/llama-2-coder-7b.Q4_K_M.gguf (version GGUF V2 (latest))\n",
      "llama_model_loader: - tensor    0:                token_embd.weight q4_K     [  4096, 32000,     1,     1 ]\n",
      "llama_model_loader: - tensor    1:              blk.0.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor    2:              blk.0.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor    3:              blk.0.attn_v.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor    4:         blk.0.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor    5:            blk.0.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor    6:              blk.0.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor    7:            blk.0.ffn_down.weight q6_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor    8:           blk.0.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor    9:            blk.0.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   10:              blk.1.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   11:              blk.1.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   12:              blk.1.attn_v.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   13:         blk.1.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   14:            blk.1.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   15:              blk.1.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   16:            blk.1.ffn_down.weight q6_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   17:           blk.1.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   18:            blk.1.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   19:              blk.2.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   20:              blk.2.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   21:              blk.2.attn_v.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   22:         blk.2.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   23:            blk.2.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   24:              blk.2.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   25:            blk.2.ffn_down.weight q6_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   26:           blk.2.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   27:            blk.2.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   28:              blk.3.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   29:              blk.3.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   30:              blk.3.attn_v.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   31:         blk.3.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   32:            blk.3.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   33:              blk.3.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   34:            blk.3.ffn_down.weight q6_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   35:           blk.3.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   36:            blk.3.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   37:              blk.4.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   38:              blk.4.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   39:              blk.4.attn_v.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   40:         blk.4.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   41:            blk.4.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   42:              blk.4.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   43:            blk.4.ffn_down.weight q4_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   44:           blk.4.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   45:            blk.4.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   46:              blk.5.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   47:              blk.5.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   48:              blk.5.attn_v.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   49:         blk.5.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   50:            blk.5.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   51:              blk.5.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   52:            blk.5.ffn_down.weight q4_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   53:           blk.5.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   54:            blk.5.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   55:              blk.6.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   56:              blk.6.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   57:              blk.6.attn_v.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   58:         blk.6.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   59:            blk.6.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   60:              blk.6.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   61:            blk.6.ffn_down.weight q6_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   62:           blk.6.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   63:            blk.6.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   64:              blk.7.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   65:              blk.7.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   66:              blk.7.attn_v.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   67:         blk.7.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   68:            blk.7.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   69:              blk.7.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   70:            blk.7.ffn_down.weight q4_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   71:           blk.7.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   72:            blk.7.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   73:              blk.8.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   74:              blk.8.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   75:              blk.8.attn_v.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   76:         blk.8.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   77:            blk.8.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   78:              blk.8.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   79:            blk.8.ffn_down.weight q4_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   80:           blk.8.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   81:            blk.8.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   82:              blk.9.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   83:              blk.9.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   84:              blk.9.attn_v.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   85:         blk.9.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   86:            blk.9.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   87:              blk.9.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   88:            blk.9.ffn_down.weight q6_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   89:           blk.9.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   90:            blk.9.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   91:             blk.10.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   92:             blk.10.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   93:             blk.10.attn_v.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   94:        blk.10.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   95:           blk.10.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   96:             blk.10.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   97:           blk.10.ffn_down.weight q4_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   98:          blk.10.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   99:           blk.10.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  100:             blk.11.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  101:             blk.11.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  102:             blk.11.attn_v.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  103:        blk.11.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  104:           blk.11.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  105:             blk.11.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  106:           blk.11.ffn_down.weight q4_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  107:          blk.11.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  108:           blk.11.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  109:             blk.12.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  110:             blk.12.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  111:             blk.12.attn_v.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  112:        blk.12.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  113:           blk.12.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  114:             blk.12.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  115:           blk.12.ffn_down.weight q6_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  116:          blk.12.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  117:           blk.12.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  118:             blk.13.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  119:             blk.13.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  120:             blk.13.attn_v.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  121:        blk.13.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  122:           blk.13.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  123:             blk.13.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  124:           blk.13.ffn_down.weight q4_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  125:          blk.13.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  126:           blk.13.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  127:             blk.14.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  128:             blk.14.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  129:             blk.14.attn_v.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  130:        blk.14.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  131:           blk.14.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  132:             blk.14.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  133:           blk.14.ffn_down.weight q4_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  134:          blk.14.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  135:           blk.14.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  136:             blk.15.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  137:             blk.15.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  138:             blk.15.attn_v.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  139:        blk.15.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  140:           blk.15.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  141:             blk.15.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  142:           blk.15.ffn_down.weight q6_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  143:          blk.15.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  144:           blk.15.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  145:             blk.16.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  146:             blk.16.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  147:             blk.16.attn_v.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  148:        blk.16.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  149:           blk.16.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  150:             blk.16.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  151:           blk.16.ffn_down.weight q4_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  152:          blk.16.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  153:           blk.16.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  154:             blk.17.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  155:             blk.17.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  156:             blk.17.attn_v.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  157:        blk.17.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  158:           blk.17.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  159:             blk.17.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  160:           blk.17.ffn_down.weight q4_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  161:          blk.17.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  162:           blk.17.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  163:             blk.18.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  164:             blk.18.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  165:             blk.18.attn_v.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  166:        blk.18.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  167:           blk.18.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  168:             blk.18.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  169:           blk.18.ffn_down.weight q6_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  170:          blk.18.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  171:           blk.18.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  172:             blk.19.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  173:             blk.19.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  174:             blk.19.attn_v.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  175:        blk.19.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  176:           blk.19.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  177:             blk.19.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  178:           blk.19.ffn_down.weight q4_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  179:          blk.19.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  180:           blk.19.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  181:             blk.20.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  182:             blk.20.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  183:             blk.20.attn_v.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  184:        blk.20.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  185:           blk.20.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  186:             blk.20.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  187:           blk.20.ffn_down.weight q4_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  188:          blk.20.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  189:           blk.20.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  190:             blk.21.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  191:             blk.21.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  192:             blk.21.attn_v.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  193:        blk.21.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  194:           blk.21.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  195:             blk.21.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  196:           blk.21.ffn_down.weight q6_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  197:          blk.21.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  198:           blk.21.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  199:             blk.22.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  200:             blk.22.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  201:             blk.22.attn_v.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  202:        blk.22.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  203:           blk.22.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  204:             blk.22.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  205:           blk.22.ffn_down.weight q4_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  206:          blk.22.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  207:           blk.22.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  208:             blk.23.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  209:             blk.23.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  210:             blk.23.attn_v.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  211:        blk.23.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  212:           blk.23.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  213:             blk.23.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  214:           blk.23.ffn_down.weight q4_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  215:          blk.23.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  216:           blk.23.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  217:             blk.24.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  218:             blk.24.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  219:             blk.24.attn_v.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  220:        blk.24.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  221:           blk.24.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  222:             blk.24.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  223:           blk.24.ffn_down.weight q6_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  224:          blk.24.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  225:           blk.24.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  226:             blk.25.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  227:             blk.25.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  228:             blk.25.attn_v.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  229:        blk.25.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  230:           blk.25.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  231:             blk.25.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  232:           blk.25.ffn_down.weight q4_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  233:          blk.25.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  234:           blk.25.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  235:             blk.26.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  236:             blk.26.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  237:             blk.26.attn_v.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  238:        blk.26.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  239:           blk.26.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  240:             blk.26.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  241:           blk.26.ffn_down.weight q4_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  242:          blk.26.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  243:           blk.26.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  244:             blk.27.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  245:             blk.27.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  246:             blk.27.attn_v.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  247:        blk.27.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  248:           blk.27.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  249:             blk.27.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  250:           blk.27.ffn_down.weight q6_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  251:          blk.27.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  252:           blk.27.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  253:             blk.28.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  254:             blk.28.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  255:             blk.28.attn_v.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  256:        blk.28.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  257:           blk.28.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  258:             blk.28.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  259:           blk.28.ffn_down.weight q6_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  260:          blk.28.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  261:           blk.28.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  262:             blk.29.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  263:             blk.29.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  264:             blk.29.attn_v.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  265:        blk.29.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  266:           blk.29.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  267:             blk.29.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  268:           blk.29.ffn_down.weight q6_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  269:          blk.29.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  270:           blk.29.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  271:             blk.30.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  272:             blk.30.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  273:             blk.30.attn_v.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  274:        blk.30.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  275:           blk.30.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  276:             blk.30.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  277:           blk.30.ffn_down.weight q6_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  278:          blk.30.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  279:           blk.30.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  280:             blk.31.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  281:             blk.31.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  282:             blk.31.attn_v.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  283:        blk.31.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  284:           blk.31.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  285:             blk.31.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  286:           blk.31.ffn_down.weight q6_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  287:          blk.31.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  288:           blk.31.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  289:               output_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  290:                    output.weight q6_K     [  4096, 32000,     1,     1 ]\n",
      "llama_model_loader: - kv   0:                       general.architecture str     \n",
      "llama_model_loader: - kv   1:                               general.name str     \n",
      "llama_model_loader: - kv   2:                       llama.context_length u32     \n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32     \n",
      "llama_model_loader: - kv   4:                          llama.block_count u32     \n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32     \n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32     \n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32     \n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32     \n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32     \n",
      "llama_model_loader: - kv  10:                          general.file_type u32     \n",
      "llama_model_loader: - kv  11:                       tokenizer.ggml.model str     \n",
      "llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr     \n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr     \n",
      "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr     \n",
      "llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32     \n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32     \n",
      "llama_model_loader: - kv  17:            tokenizer.ggml.unknown_token_id u32     \n",
      "llama_model_loader: - kv  18:               general.quantization_version u32     \n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_K:  193 tensors\n",
      "llama_model_loader: - type q6_K:   33 tensors\n",
      "llm_load_print_meta: format         = GGUF V2 (latest)\n",
      "llm_load_print_meta: arch           = llama\n",
      "llm_load_print_meta: vocab type     = SPM\n",
      "llm_load_print_meta: n_vocab        = 32000\n",
      "llm_load_print_meta: n_merges       = 0\n",
      "llm_load_print_meta: n_ctx_train    = 4096\n",
      "llm_load_print_meta: n_ctx          = 2048\n",
      "llm_load_print_meta: n_embd         = 4096\n",
      "llm_load_print_meta: n_head         = 32\n",
      "llm_load_print_meta: n_head_kv      = 32\n",
      "llm_load_print_meta: n_layer        = 32\n",
      "llm_load_print_meta: n_rot          = 128\n",
      "llm_load_print_meta: n_gqa          = 1\n",
      "llm_load_print_meta: f_norm_eps     = 1.0e-05\n",
      "llm_load_print_meta: f_norm_rms_eps = 1.0e-05\n",
      "llm_load_print_meta: n_ff           = 11008\n",
      "llm_load_print_meta: freq_base      = 10000.0\n",
      "llm_load_print_meta: freq_scale     = 1\n",
      "llm_load_print_meta: model type     = 7B\n",
      "llm_load_print_meta: model ftype    = mostly Q4_K - Medium\n",
      "llm_load_print_meta: model size     = 6.74 B\n",
      "llm_load_print_meta: general.name   = LLaMA v2\n",
      "llm_load_print_meta: BOS token = 1 '<s>'\n",
      "llm_load_print_meta: EOS token = 2 '</s>'\n",
      "llm_load_print_meta: UNK token = 0 '<unk>'\n",
      "llm_load_print_meta: LF token  = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size = 3891.34 MB\n",
      "llm_load_tensors: mem required  = 3891.34 MB (+ 1024.00 MB per state)\n",
      "..................................................................................................\n",
      "llama_new_context_with_model: kv self size  = 1024.00 MB\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M1 Pro\n",
      "ggml_metal_init: picking default device: Apple M1 Pro\n",
      "ggml_metal_init: loading '/Users/nielswarncke/miniconda3/envs/textgen/lib/python3.10/site-packages/llama_cpp/ggml-metal.metal'\n",
      "ggml_metal_init: loaded kernel_add                            0x113d66390 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_add_row                        0x113d68300 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul                            0x113d68970 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_row                        0x113d69070 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_scale                          0x113d69690 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_silu                           0x113d67620 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_relu                           0x113d67810 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_gelu                           0x113d69d30 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_soft_max                       0x113d6a950 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_soft_max_4                     0x113d6afe0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_diag_mask_inf                  0x113d6b6b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_diag_mask_inf_8                0x113d6bc60 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_f32                   0x113d67b60 | th_max =  896 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_f16                   0x113d6cb00 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_0                  0x113d6d1c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_1                  0x113d6d890 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q8_0                  0x113d6df60 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q2_K                  0x113d6e630 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q3_K                  0x113d6ed00 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_K                  0x113120bd0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q5_K                  0x1131213d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q6_K                  0x113121ee0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rms_norm                       0x113122590 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_norm                           0x10460f210 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mat_f32_f32                0x104610150 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mat_f16_f32                0x104610680 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mat_f16_f32_1row           0x113d6c070 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mat_f16_f32_l4             0x113d6fad0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_0_f32               0x113d6ff00 | th_max =  896 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_1_f32               0x113d70df0 | th_max =  896 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mat_q8_0_f32               0x113d70fe0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mat_q2_K_f32               0x113d72190 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mat_q3_K_f32               0x113a486c0 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_K_f32               0x113a48d10 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mat_q5_K_f32               0x113a49f00 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mat_q6_K_f32               0x113a4a700 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_f32_f32                 0x113a4a8f0 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_f16_f32                 0x113a4aef0 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                0x113a4bff0 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                0x113d72bf0 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                0x113d730d0 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                0x113d732c0 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                0x113d74910 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                0x113d75140 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                0x113d759f0 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                0x1131218f0 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rope                           0x113d76070 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_alibi_f32                      0x113d76600 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f16                    0x113d767f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f32                    0x113d78210 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f16_f16                    0x113d78b10 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: hasUnifiedMemory              = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 10922.67 MB\n",
      "ggml_metal_init: maxTransferRate               = built-in GPU\n",
      "llama_new_context_with_model: compute buffer total size =  169.47 MB\n",
      "llama_new_context_with_model: max tensor size =   102.54 MB\n",
      "ggml_metal_add_buffer: allocated 'data            ' buffer, size =  3891.34 MB, ( 3891.84 / 10922.67)\n",
      "ggml_metal_add_buffer: allocated 'eval            ' buffer, size =     1.48 MB, ( 3893.33 / 10922.67)\n",
      "ggml_metal_add_buffer: allocated 'kv              ' buffer, size =  1026.00 MB, ( 4919.33 / 10922.67)\n",
      "ggml_metal_add_buffer: allocated 'alloc           ' buffer, size =   168.02 MB, ( 5087.34 / 10922.67)\n"
     ]
    }
   ],
   "source": [
    "import guidance\n",
    "\n",
    "# Create a LlamaCpp instance and pass the settings to it.\n",
    "# note this assumes you have llama-cpp-python>=0.1.55 installed\n",
    "llama = guidance.llms.LlamaCpp(\n",
    "    model = \"/Users/nielswarncke/models/gguf/llama-2-coder-7b.Q4_K_M.gguf\", # downloaded from: https://huggingface.co/TheBloke/Llama-2-Coder-7B-GGUF/blob/main/llama-2-coder-7b.Q4_K_M.gguf\n",
    "    tokenizer=\"mrm8488/llama-2-coder-7b\",\n",
    "    n_gpu_layers=10,\n",
    "    n_threads=12\n",
    ")\n",
    "\n",
    "# Other models would look like this:\n",
    "# model = \"../ggml-v3-models/gpt4-x-vicuna-13B.ggmlv3.q5_1.bin\"\n",
    "# tokenizer = \"TheBloke/gpt4-x-vicuna-13B-HF\"\n",
    "\n",
    "# Or like this:\n",
    "# model = \"../ggml-v3-models/Manticore-13B.ggmlv3.q5_1.bin\"\n",
    "# tokenizer = \"openaccess-ai-collective/manticore-13b\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-24T20:46:57.522736200Z",
     "start_time": "2023-05-24T20:46:57.509223300Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div id=\"guidance-stop-button-a3977a3f-f87d-4408-b514-e235b39220d9\" style=\"cursor: pointer; margin: 0px; display: none; float: right; padding: 3px; border-radius: 4px 4px 4px 4px; border: 0px solid rgba(127, 127, 127, 1); padding-left: 10px; padding-right: 10px; font-size: 13px; background-color: rgba(127, 127, 127, 0.25);\">Stop program</div><div id=\"guidance-content-a3977a3f-f87d-4408-b514-e235b39220d9\"><pre style='margin: 0px; padding: 0px; padding-left: 8px; margin-left: -8px; border-radius: 0px; border-left: 1px solid rgba(127, 127, 127, 0.2); white-space: pre-wrap; font-family: ColfaxAI, Arial; font-size: 15px; line-height: 23px;'>The following is a character profile for an RPG game in JSON format.\n",
       "```json\n",
       "{\n",
       "    &quot;id&quot;: &quot;<span style='background-color: rgba(0, 138.56128016, 250.76166089, 0.25); display: inline;' title='{{id}}'>e1f491f7-7ab8-4dac-8c20-c92b5e7d883d</span>&quot;,\n",
       "    &quot;description&quot;: &quot;<span style='background-color: rgba(0, 138.56128016, 250.76166089, 0.25); display: inline;' title='{{description}}'>A quick and nimble fighter.</span>&quot;,\n",
       "    &quot;name&quot;: &quot;<span style='background-color: rgba(0, 165, 0, 0.25); opacity: 1.0; display: inline;' title='{{gen &#x27;name&#x27; temperature=0.7}}'></pre></div>\n",
       "<script type=\"text/javascript\">(()=>{var t={296:(t,e,n)=>{var i=NaN,o=\"[object Symbol]\",r=/^\\s+|\\s+$/g,a=/^[-+]0x[0-9a-f]+$/i,s=/^0b[01]+$/i,c=/^0o[0-7]+$/i,d=parseInt,u=\"object\"==typeof n.g&&n.g&&n.g.Object===Object&&n.g,l=\"object\"==typeof self&&self&&self.Object===Object&&self,f=u||l||Function(\"return this\")(),h=Object.prototype.toString,p=Math.max,m=Math.min,g=function(){return f.Date.now()};function b(t){var e=typeof t;return!!t&&(\"object\"==e||\"function\"==e)}function y(t){if(\"number\"==typeof t)return t;if(function(t){return\"symbol\"==typeof t||function(t){return!!t&&\"object\"==typeof t}(t)&&h.call(t)==o}(t))return i;if(b(t)){var e=\"function\"==typeof t.valueOf?t.valueOf():t;t=b(e)?e+\"\":e}if(\"string\"!=typeof t)return 0===t?t:+t;t=t.replace(r,\"\");var n=s.test(t);return n||c.test(t)?d(t.slice(2),n?2:8):a.test(t)?i:+t}t.exports=function(t,e,n){var i,o,r,a,s,c,d=0,u=!1,l=!1,f=!0;if(\"function\"!=typeof t)throw new TypeError(\"Expected a function\");function h(e){var n=i,r=o;return i=o=void 0,d=e,a=t.apply(r,n)}function v(t){var n=t-c;return void 0===c||n>=e||n<0||l&&t-d>=r}function _(){var t=g();if(v(t))return w(t);s=setTimeout(_,function(t){var n=e-(t-c);return l?m(n,r-(t-d)):n}(t))}function w(t){return s=void 0,f&&i?h(t):(i=o=void 0,a)}function j(){var t=g(),n=v(t);if(i=arguments,o=this,c=t,n){if(void 0===s)return function(t){return d=t,s=setTimeout(_,e),u?h(t):a}(c);if(l)return s=setTimeout(_,e),h(c)}return void 0===s&&(s=setTimeout(_,e)),a}return e=y(e)||0,b(n)&&(u=!!n.leading,r=(l=\"maxWait\"in n)?p(y(n.maxWait)||0,e):r,f=\"trailing\"in n?!!n.trailing:f),j.cancel=function(){void 0!==s&&clearTimeout(s),d=0,i=c=o=s=void 0},j.flush=function(){return void 0===s?a:w(g())},j}},777:t=>{var e,n,i=Math.max,o=(e=function(t,e){return function(t,e,n){if(\"function\"!=typeof t)throw new TypeError(\"Expected a function\");return setTimeout((function(){t.apply(void 0,n)}),1)}(t,0,e)},n=i(void 0===n?e.length-1:n,0),function(){for(var t=arguments,o=-1,r=i(t.length-n,0),a=Array(r);++o<r;)a[o]=t[n+o];o=-1;for(var s=Array(n+1);++o<n;)s[o]=t[o];return s[n]=a,function(t,e,n){switch(n.length){case 0:return t.call(e);case 1:return t.call(e,n[0]);case 2:return t.call(e,n[0],n[1]);case 3:return t.call(e,n[0],n[1],n[2])}return t.apply(e,n)}(e,this,s)});t.exports=o}},e={};function n(i){var o=e[i];if(void 0!==o)return o.exports;var r=e[i]={exports:{}};return t[i](r,r.exports,n),r.exports}n.n=t=>{var e=t&&t.__esModule?()=>t.default:()=>t;return n.d(e,{a:e}),e},n.d=(t,e)=>{for(var i in e)n.o(e,i)&&!n.o(t,i)&&Object.defineProperty(t,i,{enumerable:!0,get:e[i]})},n.g=function(){if(\"object\"==typeof globalThis)return globalThis;try{return this||new Function(\"return this\")()}catch(t){if(\"object\"==typeof window)return window}}(),n.o=(t,e)=>Object.prototype.hasOwnProperty.call(t,e),(()=>{\"use strict\";const t=t=>{const e=new Set;do{for(const n of Reflect.ownKeys(t))e.add([t,n])}while((t=Reflect.getPrototypeOf(t))&&t!==Object.prototype);return e};function e(e,{include:n,exclude:i}={}){const o=t=>{const e=e=>\"string\"==typeof e?t===e:e.test(t);return n?n.some(e):!i||!i.some(e)};for(const[n,i]of t(e.constructor.prototype)){if(\"constructor\"===i||!o(i))continue;const t=Reflect.getOwnPropertyDescriptor(n,i);t&&\"function\"==typeof t.value&&(e[i]=e[i].bind(e))}return e}var i=n(777),o=n.n(i),r=n(296),a=n.n(r);class s{constructor(t,n){e(this),this.interfaceId=t,this.callbackMap={},this.data={},this.pendingData={},this.jcomm=new c(\"guidance_interface_target_\"+this.interfaceId,this.updateData,\"open\"),this.debouncedSendPendingData500=a()(this.sendPendingData,500),this.debouncedSendPendingData1000=a()(this.sendPendingData,1e3),n&&o()(n)}send(t,e){this.addPendingData(t,e),this.sendPendingData()}sendEvent(t){for(const e of Object.keys(t))this.addPendingData(e,t[e]);this.sendPendingData()}debouncedSendEvent500(t){for(const e of Object.keys(t))this.addPendingData(e,t[e]);this.debouncedSendPendingData500()}debouncedSend500(t,e){this.addPendingData(t,e),this.debouncedSendPendingData500()}debouncedSend1000(t,e){this.addPendingData(t,e),this.debouncedSendPendingData1000()}addPendingData(t,e){Array.isArray(t)||(t=[t]);for(const n in t)this.pendingData[t[n]]=e}updateData(t){t=JSON.parse(t.data);for(const e in t)this.data[e]=t[e];for(const e in t)e in this.callbackMap&&this.callbackMap[e](this.data[e])}subscribe(t,e){this.callbackMap[t]=e,o()((e=>this.callbackMap[t](this.data[t])))}sendPendingData(){this.jcomm.send_data(this.pendingData),this.pendingData={}}}class c{constructor(t,e,n=\"open\"){this._fire_callback=this._fire_callback.bind(this),this._register=this._register.bind(this),this.jcomm=void 0,this.callback=e,void 0!==window.Jupyter?\"register\"===n?Jupyter.notebook.kernel.comm_manager.register_target(t,this._register):(this.jcomm=Jupyter.notebook.kernel.comm_manager.new_comm(t),this.jcomm.on_msg(this._fire_callback)):void 0!==window._mgr&&(\"register\"===n?window._mgr.widgetManager.proxyKernel.registerCommTarget(t,this._register):(this.jcomm=window._mgr.widgetManager.proxyKernel.createComm(t),this.jcomm.open({},\"\"),this.jcomm.onMsg=this._fire_callback))}send_data(t){void 0!==this.jcomm?this.jcomm.send(t):console.error(\"Jupyter comm module not yet loaded! So we can't send the message.\")}_register(t,e){this.jcomm=t,this.jcomm.on_msg(this._fire_callback)}_fire_callback(t){this.callback(t.content.data)}}class d{constructor(t,n){e(this),this.id=t,this.comm=new s(t),this.comm.subscribe(\"append\",this.appendData),this.comm.subscribe(\"replace\",this.replaceData),this.comm.subscribe(\"event\",this.eventOccurred),this.element=document.getElementById(\"guidance-content-\"+t),this.stop_button=document.getElementById(\"guidance-stop-button-\"+t),this.stop_button.onclick=()=>this.comm.send(\"event\",\"stop\")}appendData(t){t&&(this.stop_button.style.display=\"inline-block\",this.element.innerHTML+=t)}replaceData(t){t&&(this.stop_button.style.display=\"inline-block\",this.element.innerHTML=t)}eventOccurred(t){\"complete\"===t&&(this.stop_button.style.display=\"none\")}}window._guidanceDisplay=function(t,e){return new d(t,e)}})()})();; window._guidanceDisplay(\"a3977a3f-f87d-4408-b514-e235b39220d9\");</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'chainmail': -1000, 'leather': -1000, 'plate': -1000}\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/nielswarncke/Documents/agi/guidance/guidance/_program_executor.py\", line 102, in run\n",
      "    await self.visit(self.parse_tree)\n",
      "  File \"/Users/nielswarncke/Documents/agi/guidance/guidance/_program_executor.py\", line 437, in visit\n",
      "    visited_children.append(await self.visit(child, inner_next_node, inner_next_next_node, inner_prev_node, node, parent_node))\n",
      "  File \"/Users/nielswarncke/Documents/agi/guidance/guidance/_program_executor.py\", line 437, in visit\n",
      "    visited_children.append(await self.visit(child, inner_next_node, inner_next_next_node, inner_prev_node, node, parent_node))\n",
      "  File \"/Users/nielswarncke/Documents/agi/guidance/guidance/_program_executor.py\", line 404, in visit\n",
      "    command_output = await command_function(*positional_args, **named_args)\n",
      "  File \"/Users/nielswarncke/Documents/agi/guidance/guidance/library/_select.py\", line 177, in select\n",
      "    raise ValueError(\"No valid option generated in #select! Please post a GitHub issue since this should not happen :)\")\n",
      "ValueError: No valid option generated in #select! Please post a GitHub issue since this should not happen :)\n",
      "\n",
      "Error in program:  No valid option generated in #select! Please post a GitHub issue since this should not happen :)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "No valid option generated in #select! Please post a GitHub issue since this should not happen :)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 22\u001b[0m\n\u001b[1;32m      6\u001b[0m character_maker \u001b[38;5;241m=\u001b[39m guidance(\u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;124mThe following is a character profile for an RPG game in JSON format.\u001b[39m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;124m```json\u001b[39m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;124m{\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;124m    \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mitems\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m: [\u001b[39m\u001b[38;5;124m{{\u001b[39m\u001b[38;5;124m#geneach \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mitems\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m num_iterations=5 join=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m}}\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m{{\u001b[39m\u001b[38;5;124mgen \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mthis\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m temperature=0.7}}\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m{{\u001b[39m\u001b[38;5;124m/geneach}}]\u001b[39m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;124m}```\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m)\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# generate a character\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m char \u001b[38;5;241m=\u001b[39m \u001b[43mcharacter_maker\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mid\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43me1f491f7-7ab8-4dac-8c20-c92b5e7d883d\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdescription\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mA quick and nimble fighter.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalid_weapons\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalid_weapons\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mllm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mllama\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaching\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[1;32m     26\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/agi/guidance/guidance/_program.py:234\u001b[0m, in \u001b[0;36mProgram.__call__\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    232\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stream_run(loop, new_program)\n\u001b[1;32m    233\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 234\u001b[0m         \u001b[43mloop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_until_complete\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_program\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    236\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m new_program\n",
      "File \u001b[0;32m~/miniconda3/envs/textgen/lib/python3.10/site-packages/nest_asyncio.py:99\u001b[0m, in \u001b[0;36m_patch_loop.<locals>.run_until_complete\u001b[0;34m(self, future)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m f\u001b[38;5;241m.\u001b[39mdone():\n\u001b[1;32m     97\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m     98\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEvent loop stopped before Future completed.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 99\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/textgen/lib/python3.10/asyncio/futures.py:201\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__log_traceback \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    200\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 201\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\u001b[38;5;241m.\u001b[39mwith_traceback(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception_tb)\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result\n",
      "File \u001b[0;32m~/miniconda3/envs/textgen/lib/python3.10/asyncio/tasks.py:232\u001b[0m, in \u001b[0;36mTask.__step\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    229\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m exc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    230\u001b[0m         \u001b[38;5;66;03m# We use the `send` method directly, because coroutines\u001b[39;00m\n\u001b[1;32m    231\u001b[0m         \u001b[38;5;66;03m# don't have `__iter__` and `__next__` methods.\u001b[39;00m\n\u001b[0;32m--> 232\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[43mcoro\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    233\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    234\u001b[0m         result \u001b[38;5;241m=\u001b[39m coro\u001b[38;5;241m.\u001b[39mthrow(exc)\n",
      "File \u001b[0;32m~/Documents/agi/guidance/guidance/_program.py:385\u001b[0m, in \u001b[0;36mProgram.execute\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    383\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    384\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm\u001b[38;5;241m.\u001b[39msession(asynchronous\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m llm_session:\n\u001b[0;32m--> 385\u001b[0m         \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_executor\u001b[38;5;241m.\u001b[39mrun(llm_session)\n\u001b[1;32m    386\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_text \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_executor\u001b[38;5;241m.\u001b[39mprefix\n\u001b[1;32m    388\u001b[0m \u001b[38;5;66;03m# delete the executor and so mark the program as not executing\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/agi/guidance/guidance/_program_executor.py:106\u001b[0m, in \u001b[0;36mProgramExecutor.run\u001b[0;34m(self, llm_session)\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;28mprint\u001b[39m(traceback\u001b[38;5;241m.\u001b[39mformat_exc())\n\u001b[1;32m    105\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError in program: \u001b[39m\u001b[38;5;124m\"\u001b[39m, e)\n\u001b[0;32m--> 106\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "File \u001b[0;32m~/Documents/agi/guidance/guidance/_program_executor.py:102\u001b[0m, in \u001b[0;36mProgramExecutor.run\u001b[0;34m(self, llm_session)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm_session \u001b[38;5;241m=\u001b[39m llm_session\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     98\u001b[0m     \u001b[38;5;66;03m# first parse all the whitespace control\u001b[39;00m\n\u001b[1;32m     99\u001b[0m     \u001b[38;5;66;03m# self.whitespace_control_visit(self.parse_tree)\u001b[39;00m\n\u001b[1;32m    100\u001b[0m \n\u001b[1;32m    101\u001b[0m     \u001b[38;5;66;03m# now execute the program\u001b[39;00m\n\u001b[0;32m--> 102\u001b[0m     \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvisit(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparse_tree)\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    104\u001b[0m     \u001b[38;5;28mprint\u001b[39m(traceback\u001b[38;5;241m.\u001b[39mformat_exc())\n",
      "File \u001b[0;32m~/Documents/agi/guidance/guidance/_program_executor.py:437\u001b[0m, in \u001b[0;36mProgramExecutor.visit\u001b[0;34m(self, node, next_node, next_next_node, prev_node, parent_node, grandparent_node)\u001b[0m\n\u001b[1;32m    435\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    436\u001b[0m         inner_prev_node \u001b[38;5;241m=\u001b[39m prev_node\n\u001b[0;32m--> 437\u001b[0m     visited_children\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvisit(child, inner_next_node, inner_next_next_node, inner_prev_node, node, parent_node))\n\u001b[1;32m    438\u001b[0m \u001b[38;5;66;03m# visited_children = [self.visit(child) for child in node.children]\u001b[39;00m\n\u001b[1;32m    440\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(visited_children) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m~/Documents/agi/guidance/guidance/_program_executor.py:437\u001b[0m, in \u001b[0;36mProgramExecutor.visit\u001b[0;34m(self, node, next_node, next_next_node, prev_node, parent_node, grandparent_node)\u001b[0m\n\u001b[1;32m    435\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    436\u001b[0m         inner_prev_node \u001b[38;5;241m=\u001b[39m prev_node\n\u001b[0;32m--> 437\u001b[0m     visited_children\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvisit(child, inner_next_node, inner_next_next_node, inner_prev_node, node, parent_node))\n\u001b[1;32m    438\u001b[0m \u001b[38;5;66;03m# visited_children = [self.visit(child) for child in node.children]\u001b[39;00m\n\u001b[1;32m    440\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(visited_children) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m~/Documents/agi/guidance/guidance/_program_executor.py:404\u001b[0m, in \u001b[0;36mProgramExecutor.visit\u001b[0;34m(self, node, next_node, next_next_node, prev_node, parent_node, grandparent_node)\u001b[0m\n\u001b[1;32m    402\u001b[0m \u001b[38;5;66;03m# call the optionally asyncronous command\u001b[39;00m\n\u001b[1;32m    403\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39miscoroutinefunction(command_function):\n\u001b[0;32m--> 404\u001b[0m     command_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m command_function(\u001b[38;5;241m*\u001b[39mpositional_args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mnamed_args)\n\u001b[1;32m    405\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    406\u001b[0m     command_output \u001b[38;5;241m=\u001b[39m command_function(\u001b[38;5;241m*\u001b[39mpositional_args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mnamed_args)\n",
      "File \u001b[0;32m~/Documents/agi/guidance/guidance/library/_select.py:177\u001b[0m, in \u001b[0;36mselect\u001b[0;34m(variable_name, options, logprobs, list_append, _parser_context)\u001b[0m\n\u001b[1;32m    175\u001b[0m     pprint(option_logprobs)\n\u001b[1;32m    176\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m, flush\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m--> 177\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo valid option generated in #select! Please post a GitHub issue since this should not happen :)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    179\u001b[0m partial_output(selected_option)\n",
      "\u001b[0;31mValueError\u001b[0m: No valid option generated in #select! Please post a GitHub issue since this should not happen :)"
     ]
    }
   ],
   "source": [
    "# 1st Example\n",
    "# we can pre-define valid option sets\n",
    "valid_weapons = [\"sword\", \"axe\", \"mace\", \"spear\", \"bow\", \"crossbow\"]\n",
    "\n",
    "# define the prompt\n",
    "character_maker = guidance(\"\"\"The following is a character profile for an RPG game in JSON format.\n",
    "```json\n",
    "{\n",
    "    \"id\": \"{{id}}\",\n",
    "    \"description\": \"{{description}}\",\n",
    "    \"name\": \"{{gen 'name' temperature=0.7}}\",\n",
    "    \"age\": {{gen 'age' pattern='[0-9]+' max_tokens=2 stop=','}},\n",
    "    \"armor\": \"{{#select 'armor'}}leather{{or}}chainmail{{or}}plate{{/select}}\",\n",
    "    \"weapon\": \"{{select 'weapon' options=valid_weapons}}\",\n",
    "    \"class\": \"{{gen 'class'}}\",\n",
    "    \"mantra\": \"{{gen 'mantra' temperature=0.7}}\",\n",
    "    \"strength\": {{gen 'strength' pattern='[0-9]+' max_tokens=2 stop=','}},\n",
    "    \"items\": [{{#geneach 'items' num_iterations=5 join=', '}}\"{{gen 'this' temperature=0.7}}\"{{/geneach}}]\n",
    "}```\"\"\")\n",
    "\n",
    "# generate a character\n",
    "char = character_maker(\n",
    "    id=\"e1f491f7-7ab8-4dac-8c20-c92b5e7d883d\",\n",
    "    description=\"A quick and nimble fighter.\",\n",
    "    valid_weapons=valid_weapons, llm=llama, caching=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# 2nd Example\n",
    "# define a recursive function that takes a jsonformer schema and returns a guidance program\n",
    "def jsonformer2guidance(schema, key=None, indent=0):\n",
    "    out = \"\"\n",
    "    if schema['type'] == 'object':\n",
    "        out += \"  \"*indent + \"{\\n\"\n",
    "        for k,v in schema['properties'].items():\n",
    "            out += \"  \"*(indent+1) + k + \": \" + jsonformer2guidance(v, k, indent+1) + \",\\n\"\n",
    "        out += \"  \"*indent + \"}\"\n",
    "        return out\n",
    "    elif schema['type'] == 'array':\n",
    "        if 'max_items' in schema:\n",
    "            extra_args = f\" max_iterations={schema['max_items']}\"\n",
    "        else:\n",
    "            extra_args = \"\"\n",
    "        return \"[{{#geneach '\"+key+\"' stop=']'\"+extra_args+\"}}{{#unless @first}}, {{/unless}}\" + jsonformer2guidance(schema['items'], \"this\") + \"{{/geneach}}]\"\n",
    "    elif schema['type'] == 'string':\n",
    "        return \"\\\"{{gen '\"+key+\"' stop='\\\"'}}\\\"\"\n",
    "    elif schema['type'] == 'number':\n",
    "        return \"{{gen '\"+key+\"' pattern='[0-9\\\\.]' stop=','}}\"\n",
    "    elif schema['type'] == 'boolean':\n",
    "        return \"{{#select '\"+key+\"'}}True{{or}}False{{/select}}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# define a jsonformer schema\n",
    "json_schema = {\n",
    "    \"type\": \"object\",\n",
    "    \"properties\": {\n",
    "        \"name\": {\"type\": \"string\"},\n",
    "        \"age\": {\"type\": \"number\"},\n",
    "        \"is_student\": {\"type\": \"boolean\"},\n",
    "        \"courses\": {\n",
    "            \"type\": \"array\",\n",
    "            \"max_items\": 6,\n",
    "            \"items\": {\"type\": \"string\"}\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method LlamaCpp.decode of <guidance.llms._llama_cpp.LlamaCpp object at 0x10428c9d0>>\n",
      "<bound method LlamaCpp.decode of <guidance.llms._llama_cpp.LlamaCpp object at 0x10428c9d0>>\n",
      "<bound method LlamaCpp.decode of <guidance.llms._llama_cpp.LlamaCpp object at 0x10428c9d0>>\n",
      "<bound method LlamaCpp.decode of <guidance.llms._llama_cpp.LlamaCpp object at 0x10428c9d0>>\n",
      "<bound method LlamaCpp.decode of <guidance.llms._llama_cpp.LlamaCpp object at 0x10428c9d0>>\n",
      "<bound method LlamaCpp.decode of <guidance.llms._llama_cpp.LlamaCpp object at 0x10428c9d0>>\n",
      "<bound method LlamaCpp.decode of <guidance.llms._llama_cpp.LlamaCpp object at 0x10428c9d0>>\n",
      "<bound method LlamaCpp.decode of <guidance.llms._llama_cpp.LlamaCpp object at 0x10428c9d0>>\n",
      "<bound method LlamaCpp.decode of <guidance.llms._llama_cpp.LlamaCpp object at 0x10428c9d0>>\n",
      "<bound method LlamaCpp.decode of <guidance.llms._llama_cpp.LlamaCpp object at 0x10428c9d0>>\n",
      "<bound method LlamaCpp.decode of <guidance.llms._llama_cpp.LlamaCpp object at 0x10428c9d0>>\n",
      "<bound method LlamaCpp.decode of <guidance.llms._llama_cpp.LlamaCpp object at 0x10428c9d0>>\n",
      "{'False': -1000, 'True': -1000}\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/nielswarncke/Documents/agi/guidance/guidance/_program_executor.py\", line 102, in run\n",
      "    await self.visit(self.parse_tree)\n",
      "  File \"/Users/nielswarncke/Documents/agi/guidance/guidance/_program_executor.py\", line 437, in visit\n",
      "    visited_children.append(await self.visit(child, inner_next_node, inner_next_next_node, inner_prev_node, node, parent_node))\n",
      "  File \"/Users/nielswarncke/Documents/agi/guidance/guidance/_program_executor.py\", line 437, in visit\n",
      "    visited_children.append(await self.visit(child, inner_next_node, inner_next_next_node, inner_prev_node, node, parent_node))\n",
      "  File \"/Users/nielswarncke/Documents/agi/guidance/guidance/_program_executor.py\", line 404, in visit\n",
      "    command_output = await command_function(*positional_args, **named_args)\n",
      "  File \"/Users/nielswarncke/Documents/agi/guidance/guidance/library/_select.py\", line 177, in select\n",
      "    raise ValueError(\"No valid option generated in #select! Please post a GitHub issue since this should not happen :)\")\n",
      "ValueError: No valid option generated in #select! Please post a GitHub issue since this should not happen :)\n",
      "\n",
      "Error in program:  No valid option generated in #select! Please post a GitHub issue since this should not happen :)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "No valid option generated in #select! Please post a GitHub issue since this should not happen :)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGenerate a person\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms information based on the following schema:\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      3\u001b[0m program \u001b[38;5;241m=\u001b[39m guidance(prompt \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m jsonformer2guidance(json_schema))\n\u001b[0;32m----> 4\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mprogram\u001b[49m\u001b[43m(\u001b[49m\u001b[43mllm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mllama\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msilent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(out)\n",
      "File \u001b[0;32m~/Documents/agi/guidance/guidance/_program.py:234\u001b[0m, in \u001b[0;36mProgram.__call__\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    232\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stream_run(loop, new_program)\n\u001b[1;32m    233\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 234\u001b[0m         \u001b[43mloop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_until_complete\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_program\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    236\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m new_program\n",
      "File \u001b[0;32m~/miniconda3/envs/textgen/lib/python3.10/site-packages/nest_asyncio.py:99\u001b[0m, in \u001b[0;36m_patch_loop.<locals>.run_until_complete\u001b[0;34m(self, future)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m f\u001b[38;5;241m.\u001b[39mdone():\n\u001b[1;32m     97\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m     98\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEvent loop stopped before Future completed.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 99\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/textgen/lib/python3.10/asyncio/futures.py:201\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__log_traceback \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    200\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 201\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\u001b[38;5;241m.\u001b[39mwith_traceback(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception_tb)\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result\n",
      "File \u001b[0;32m~/miniconda3/envs/textgen/lib/python3.10/asyncio/tasks.py:232\u001b[0m, in \u001b[0;36mTask.__step\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    229\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m exc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    230\u001b[0m         \u001b[38;5;66;03m# We use the `send` method directly, because coroutines\u001b[39;00m\n\u001b[1;32m    231\u001b[0m         \u001b[38;5;66;03m# don't have `__iter__` and `__next__` methods.\u001b[39;00m\n\u001b[0;32m--> 232\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[43mcoro\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    233\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    234\u001b[0m         result \u001b[38;5;241m=\u001b[39m coro\u001b[38;5;241m.\u001b[39mthrow(exc)\n",
      "File \u001b[0;32m~/Documents/agi/guidance/guidance/_program.py:385\u001b[0m, in \u001b[0;36mProgram.execute\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    383\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    384\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm\u001b[38;5;241m.\u001b[39msession(asynchronous\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m llm_session:\n\u001b[0;32m--> 385\u001b[0m         \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_executor\u001b[38;5;241m.\u001b[39mrun(llm_session)\n\u001b[1;32m    386\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_text \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_executor\u001b[38;5;241m.\u001b[39mprefix\n\u001b[1;32m    388\u001b[0m \u001b[38;5;66;03m# delete the executor and so mark the program as not executing\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/agi/guidance/guidance/_program_executor.py:106\u001b[0m, in \u001b[0;36mProgramExecutor.run\u001b[0;34m(self, llm_session)\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;28mprint\u001b[39m(traceback\u001b[38;5;241m.\u001b[39mformat_exc())\n\u001b[1;32m    105\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError in program: \u001b[39m\u001b[38;5;124m\"\u001b[39m, e)\n\u001b[0;32m--> 106\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "File \u001b[0;32m~/Documents/agi/guidance/guidance/_program_executor.py:102\u001b[0m, in \u001b[0;36mProgramExecutor.run\u001b[0;34m(self, llm_session)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm_session \u001b[38;5;241m=\u001b[39m llm_session\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     98\u001b[0m     \u001b[38;5;66;03m# first parse all the whitespace control\u001b[39;00m\n\u001b[1;32m     99\u001b[0m     \u001b[38;5;66;03m# self.whitespace_control_visit(self.parse_tree)\u001b[39;00m\n\u001b[1;32m    100\u001b[0m \n\u001b[1;32m    101\u001b[0m     \u001b[38;5;66;03m# now execute the program\u001b[39;00m\n\u001b[0;32m--> 102\u001b[0m     \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvisit(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparse_tree)\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    104\u001b[0m     \u001b[38;5;28mprint\u001b[39m(traceback\u001b[38;5;241m.\u001b[39mformat_exc())\n",
      "File \u001b[0;32m~/Documents/agi/guidance/guidance/_program_executor.py:437\u001b[0m, in \u001b[0;36mProgramExecutor.visit\u001b[0;34m(self, node, next_node, next_next_node, prev_node, parent_node, grandparent_node)\u001b[0m\n\u001b[1;32m    435\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    436\u001b[0m         inner_prev_node \u001b[38;5;241m=\u001b[39m prev_node\n\u001b[0;32m--> 437\u001b[0m     visited_children\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvisit(child, inner_next_node, inner_next_next_node, inner_prev_node, node, parent_node))\n\u001b[1;32m    438\u001b[0m \u001b[38;5;66;03m# visited_children = [self.visit(child) for child in node.children]\u001b[39;00m\n\u001b[1;32m    440\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(visited_children) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m~/Documents/agi/guidance/guidance/_program_executor.py:437\u001b[0m, in \u001b[0;36mProgramExecutor.visit\u001b[0;34m(self, node, next_node, next_next_node, prev_node, parent_node, grandparent_node)\u001b[0m\n\u001b[1;32m    435\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    436\u001b[0m         inner_prev_node \u001b[38;5;241m=\u001b[39m prev_node\n\u001b[0;32m--> 437\u001b[0m     visited_children\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvisit(child, inner_next_node, inner_next_next_node, inner_prev_node, node, parent_node))\n\u001b[1;32m    438\u001b[0m \u001b[38;5;66;03m# visited_children = [self.visit(child) for child in node.children]\u001b[39;00m\n\u001b[1;32m    440\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(visited_children) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m~/Documents/agi/guidance/guidance/_program_executor.py:404\u001b[0m, in \u001b[0;36mProgramExecutor.visit\u001b[0;34m(self, node, next_node, next_next_node, prev_node, parent_node, grandparent_node)\u001b[0m\n\u001b[1;32m    402\u001b[0m \u001b[38;5;66;03m# call the optionally asyncronous command\u001b[39;00m\n\u001b[1;32m    403\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39miscoroutinefunction(command_function):\n\u001b[0;32m--> 404\u001b[0m     command_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m command_function(\u001b[38;5;241m*\u001b[39mpositional_args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mnamed_args)\n\u001b[1;32m    405\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    406\u001b[0m     command_output \u001b[38;5;241m=\u001b[39m command_function(\u001b[38;5;241m*\u001b[39mpositional_args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mnamed_args)\n",
      "File \u001b[0;32m~/Documents/agi/guidance/guidance/library/_select.py:177\u001b[0m, in \u001b[0;36mselect\u001b[0;34m(variable_name, options, logprobs, list_append, _parser_context)\u001b[0m\n\u001b[1;32m    175\u001b[0m     pprint(option_logprobs)\n\u001b[1;32m    176\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m, flush\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m--> 177\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo valid option generated in #select! Please post a GitHub issue since this should not happen :)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    179\u001b[0m partial_output(selected_option)\n",
      "\u001b[0;31mValueError\u001b[0m: No valid option generated in #select! Please post a GitHub issue since this should not happen :)"
     ]
    }
   ],
   "source": [
    "# run the guidance program\n",
    "prompt = \"Generate a person's information based on the following schema:\"\n",
    "program = guidance(prompt + \"\\n\" + jsonformer2guidance(json_schema))\n",
    "out = program(llm=llama, silent=True)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() got an unexpected keyword argument 'tokenizer_name'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/sclundbe/projects/guidance/notebooks/api_examples/llms/LamaCpp.ipynb Cell 12\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bgcrsandbox364.redmond.corp.microsoft.com/home/sclundbe/projects/guidance/notebooks/api_examples/llms/LamaCpp.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# 3rd Example\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bgcrsandbox364.redmond.corp.microsoft.com/home/sclundbe/projects/guidance/notebooks/api_examples/llms/LamaCpp.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39m# settings =  guidance.llms.LlamaCppSettings()\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bgcrsandbox364.redmond.corp.microsoft.com/home/sclundbe/projects/guidance/notebooks/api_examples/llms/LamaCpp.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39m# settings.model = \"../ggml-v3-models/Manticore-13B-Chat-Pyg.ggmlv3.q5_1.bin\"\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bgcrsandbox364.redmond.corp.microsoft.com/home/sclundbe/projects/guidance/notebooks/api_examples/llms/LamaCpp.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39m# settings.before_role = \"<|\"\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bgcrsandbox364.redmond.corp.microsoft.com/home/sclundbe/projects/guidance/notebooks/api_examples/llms/LamaCpp.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39m# settings.after_role = \"|>\"\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bgcrsandbox364.redmond.corp.microsoft.com/home/sclundbe/projects/guidance/notebooks/api_examples/llms/LamaCpp.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m llama \u001b[39m=\u001b[39m guidance\u001b[39m.\u001b[39;49mllms\u001b[39m.\u001b[39;49mLlamaCpp(\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgcrsandbox364.redmond.corp.microsoft.com/home/sclundbe/projects/guidance/notebooks/api_examples/llms/LamaCpp.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m     model \u001b[39m=\u001b[39;49m \u001b[39m\"\u001b[39;49m\u001b[39m../ggml-v3-models/Manticore-13B-Chat-Pyg.ggmlv3.q5_1.bin\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgcrsandbox364.redmond.corp.microsoft.com/home/sclundbe/projects/guidance/notebooks/api_examples/llms/LamaCpp.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m     n_gpu_layers \u001b[39m=\u001b[39;49m \u001b[39m10\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgcrsandbox364.redmond.corp.microsoft.com/home/sclundbe/projects/guidance/notebooks/api_examples/llms/LamaCpp.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m     n_threads \u001b[39m=\u001b[39;49m \u001b[39m12\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgcrsandbox364.redmond.corp.microsoft.com/home/sclundbe/projects/guidance/notebooks/api_examples/llms/LamaCpp.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=12'>13</a>\u001b[0m     tokenizer_name \u001b[39m=\u001b[39;49m \u001b[39m\"\u001b[39;49m\u001b[39mopenaccess-ai-collective/manticore-13b-chat-pyg\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgcrsandbox364.redmond.corp.microsoft.com/home/sclundbe/projects/guidance/notebooks/api_examples/llms/LamaCpp.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=13'>14</a>\u001b[0m     before_role \u001b[39m=\u001b[39;49m \u001b[39m\"\u001b[39;49m\u001b[39m<|\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgcrsandbox364.redmond.corp.microsoft.com/home/sclundbe/projects/guidance/notebooks/api_examples/llms/LamaCpp.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=14'>15</a>\u001b[0m     after_role \u001b[39m=\u001b[39;49m \u001b[39m\"\u001b[39;49m\u001b[39m|>\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgcrsandbox364.redmond.corp.microsoft.com/home/sclundbe/projects/guidance/notebooks/api_examples/llms/LamaCpp.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=15'>16</a>\u001b[0m     caching\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m, silent\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgcrsandbox364.redmond.corp.microsoft.com/home/sclundbe/projects/guidance/notebooks/api_examples/llms/LamaCpp.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=16'>17</a>\u001b[0m )\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'tokenizer_name'"
     ]
    }
   ],
   "source": [
    "# 3rd Example\n",
    "# settings =  guidance.llms.LlamaCppSettings()\n",
    "# settings.model = \"../ggml-v3-models/Manticore-13B-Chat-Pyg.ggmlv3.q5_1.bin\"\n",
    "# settings.n_gpu_layers = 10\n",
    "# settings.n_threads = 12\n",
    "# settings.tokenizer_name = \"openaccess-ai-collective/manticore-13b-chat-pyg\"\n",
    "# settings.before_role = \"<|\"\n",
    "# settings.after_role = \"|>\"\n",
    "llama = guidance.llms.LlamaCpp(\n",
    "    model = \"../ggml-v3-models/Manticore-13B-Chat-Pyg.ggmlv3.q5_1.bin\",\n",
    "    n_gpu_layers = 10,\n",
    "    n_threads = 12,\n",
    "    tokenizer = \"openaccess-ai-collective/manticore-13b-chat-pyg\",\n",
    "    before_role = \"<|\",\n",
    "    after_role = \"|>\",\n",
    "    caching=False, silent=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# llama = guidance.llms.LlamaCpp(settings=settings, caching=False)\n",
    "experts = guidance('''\n",
    "{{#system~}}\n",
    "You are a helpful and terse assistant.\n",
    "{{~/system}}\n",
    "\n",
    "{{#user~}}\n",
    "I want a response to the following question:\n",
    "{{query}}\n",
    "Name 3 world-class experts (past or present) who would be great at answering this?\n",
    "Don't answer the question yet.\n",
    "{{~/user}}\n",
    "\n",
    "{{#assistant~}}\n",
    "{{gen 'expert_names' temperature=0 max_tokens=300}}\n",
    "{{~/assistant}}\n",
    "\n",
    "{{#user~}}\n",
    "Great, now please answer the question as if these experts had collaborated in writing a joint anonymous answer.\n",
    "{{~/user}}\n",
    "\n",
    "{{#assistant~}}\n",
    "{{gen 'answer' temperature=0 max_tokens=500}}\n",
    "{{~/assistant}}\n",
    "''', llm=llama)\n",
    "\n",
    "\n",
    "experts = guidance('''\n",
    "{{#system~}}\n",
    "You are a helpful and terse assistant.\n",
    "{{~/system}}\n",
    "\n",
    "{{gen 'answer' temperature=0 max_tokens=500}}\n",
    "''', llm=llama)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div id=\"guidance-stop-button-94f6e980-59ae-4c42-8971-6fa8bec98498\" style=\"cursor: pointer; margin: 0px; display: none; float: right; padding: 3px; border-radius: 4px 4px 4px 4px; border: 0px solid rgba(127, 127, 127, 1); padding-left: 10px; padding-right: 10px; font-size: 13px; background-color: rgba(127, 127, 127, 0.25);\">Stop program</div><div id=\"guidance-content-94f6e980-59ae-4c42-8971-6fa8bec98498\"><pre style='margin: 0px; padding: 0px; padding-left: 8px; margin-left: -8px; border-radius: 0px; border-left: 1px solid rgba(127, 127, 127, 0.2); white-space: pre-wrap; font-family: ColfaxAI, Arial; font-size: 15px; line-height: 23px;'><div style='display: flex; border-bottom: 1px solid rgba(127, 127, 127, 0.2); align-items: center;'><div style='flex: 0 0 80px; opacity: 0.5;'>system</div><div style='flex-grow: 1; padding: 5px; padding-top: 10px; padding-bottom: 10px; margin-top: 0px; white-space: pre-wrap; margin-bottom: 0px;'>You are a helpful and terse assistant.</div></div><span style='background-color: rgba(0, 165, 0, 0.25); opacity: 1.0; display: inline;' title='{{gen &#x27;answer&#x27; temperature=0 max_tokens=500}}'>&lt;/system&gt;\n",
       "\n",
       "&lt;question&gt;What is the name of the system?\n",
       "\n",
       "&lt;/question&gt;\n",
       "\n",
       "&lt;answer&gt;The system&#x27;s name is &quot;System&quot;.\n",
       "\n",
       "&lt;/answer&gt;\n",
       "\n",
       "&lt;question&gt;How can you access the system?\n",
       "\n",
       "&lt;/question&gt;\n",
       "\n",
       "&lt;answer&gt;To access the system, use the command &quot;system&quot; followed by the desired action (e.g., &quot;system list&quot;, &quot;system create&quot;).\n",
       "\n",
       "&lt;/answer&gt;\n",
       "\n",
       "&lt;question&gt;What actions can be performed on the system?\n",
       "\n",
       "&lt;/question&gt;\n",
       "\n",
       "&lt;answer&gt;Actions that can be performed on the system include creating new systems, listing existing systems, deleting existing systems, and modifying existing systems.\n",
       "\n",
       "&lt;/answer&gt;\n",
       "\n",
       "&lt;question&gt;Is there any documentation available for the system?\n",
       "\n",
       "&lt;/question&gt;\n",
       "\n",
       "&lt;answer&gt;Yes, there is documentation available for the system. The documentation can be found at &lt;a href=&quot;https://www.example.com/documentation&quot;&gt;https://www.example.com/documentation&lt;/a&gt;.\n",
       "\n",
       "&lt;/answer&gt;\n",
       "\n",
       "&lt;question&gt;Are there any known issues with the system?\n",
       "\n",
       "&lt;/question&gt;\n",
       "\n",
       "&lt;answer&gt;No, there are no known issues with the system.\n",
       "\n",
       "&lt;/answer&gt;</span>\n",
       " </pre></div>\n",
       "<script type=\"text/javascript\">(()=>{var t={296:(t,e,n)=>{var i=NaN,o=\"[object Symbol]\",r=/^\\s+|\\s+$/g,a=/^[-+]0x[0-9a-f]+$/i,s=/^0b[01]+$/i,c=/^0o[0-7]+$/i,d=parseInt,u=\"object\"==typeof n.g&&n.g&&n.g.Object===Object&&n.g,l=\"object\"==typeof self&&self&&self.Object===Object&&self,f=u||l||Function(\"return this\")(),h=Object.prototype.toString,p=Math.max,m=Math.min,g=function(){return f.Date.now()};function b(t){var e=typeof t;return!!t&&(\"object\"==e||\"function\"==e)}function y(t){if(\"number\"==typeof t)return t;if(function(t){return\"symbol\"==typeof t||function(t){return!!t&&\"object\"==typeof t}(t)&&h.call(t)==o}(t))return i;if(b(t)){var e=\"function\"==typeof t.valueOf?t.valueOf():t;t=b(e)?e+\"\":e}if(\"string\"!=typeof t)return 0===t?t:+t;t=t.replace(r,\"\");var n=s.test(t);return n||c.test(t)?d(t.slice(2),n?2:8):a.test(t)?i:+t}t.exports=function(t,e,n){var i,o,r,a,s,c,d=0,u=!1,l=!1,f=!0;if(\"function\"!=typeof t)throw new TypeError(\"Expected a function\");function h(e){var n=i,r=o;return i=o=void 0,d=e,a=t.apply(r,n)}function v(t){var n=t-c;return void 0===c||n>=e||n<0||l&&t-d>=r}function _(){var t=g();if(v(t))return w(t);s=setTimeout(_,function(t){var n=e-(t-c);return l?m(n,r-(t-d)):n}(t))}function w(t){return s=void 0,f&&i?h(t):(i=o=void 0,a)}function j(){var t=g(),n=v(t);if(i=arguments,o=this,c=t,n){if(void 0===s)return function(t){return d=t,s=setTimeout(_,e),u?h(t):a}(c);if(l)return s=setTimeout(_,e),h(c)}return void 0===s&&(s=setTimeout(_,e)),a}return e=y(e)||0,b(n)&&(u=!!n.leading,r=(l=\"maxWait\"in n)?p(y(n.maxWait)||0,e):r,f=\"trailing\"in n?!!n.trailing:f),j.cancel=function(){void 0!==s&&clearTimeout(s),d=0,i=c=o=s=void 0},j.flush=function(){return void 0===s?a:w(g())},j}},777:t=>{var e,n,i=Math.max,o=(e=function(t,e){return function(t,e,n){if(\"function\"!=typeof t)throw new TypeError(\"Expected a function\");return setTimeout((function(){t.apply(void 0,n)}),1)}(t,0,e)},n=i(void 0===n?e.length-1:n,0),function(){for(var t=arguments,o=-1,r=i(t.length-n,0),a=Array(r);++o<r;)a[o]=t[n+o];o=-1;for(var s=Array(n+1);++o<n;)s[o]=t[o];return s[n]=a,function(t,e,n){switch(n.length){case 0:return t.call(e);case 1:return t.call(e,n[0]);case 2:return t.call(e,n[0],n[1]);case 3:return t.call(e,n[0],n[1],n[2])}return t.apply(e,n)}(e,this,s)});t.exports=o}},e={};function n(i){var o=e[i];if(void 0!==o)return o.exports;var r=e[i]={exports:{}};return t[i](r,r.exports,n),r.exports}n.n=t=>{var e=t&&t.__esModule?()=>t.default:()=>t;return n.d(e,{a:e}),e},n.d=(t,e)=>{for(var i in e)n.o(e,i)&&!n.o(t,i)&&Object.defineProperty(t,i,{enumerable:!0,get:e[i]})},n.g=function(){if(\"object\"==typeof globalThis)return globalThis;try{return this||new Function(\"return this\")()}catch(t){if(\"object\"==typeof window)return window}}(),n.o=(t,e)=>Object.prototype.hasOwnProperty.call(t,e),(()=>{\"use strict\";const t=t=>{const e=new Set;do{for(const n of Reflect.ownKeys(t))e.add([t,n])}while((t=Reflect.getPrototypeOf(t))&&t!==Object.prototype);return e};function e(e,{include:n,exclude:i}={}){const o=t=>{const e=e=>\"string\"==typeof e?t===e:e.test(t);return n?n.some(e):!i||!i.some(e)};for(const[n,i]of t(e.constructor.prototype)){if(\"constructor\"===i||!o(i))continue;const t=Reflect.getOwnPropertyDescriptor(n,i);t&&\"function\"==typeof t.value&&(e[i]=e[i].bind(e))}return e}var i=n(777),o=n.n(i),r=n(296),a=n.n(r);class s{constructor(t,n){e(this),this.interfaceId=t,this.callbackMap={},this.data={},this.pendingData={},this.jcomm=new c(\"guidance_interface_target_\"+this.interfaceId,this.updateData,\"open\"),this.debouncedSendPendingData500=a()(this.sendPendingData,500),this.debouncedSendPendingData1000=a()(this.sendPendingData,1e3),n&&o()(n)}send(t,e){this.addPendingData(t,e),this.sendPendingData()}sendEvent(t){for(const e of Object.keys(t))this.addPendingData(e,t[e]);this.sendPendingData()}debouncedSendEvent500(t){for(const e of Object.keys(t))this.addPendingData(e,t[e]);this.debouncedSendPendingData500()}debouncedSend500(t,e){this.addPendingData(t,e),this.debouncedSendPendingData500()}debouncedSend1000(t,e){this.addPendingData(t,e),this.debouncedSendPendingData1000()}addPendingData(t,e){Array.isArray(t)||(t=[t]);for(const n in t)this.pendingData[t[n]]=e}updateData(t){t=JSON.parse(t.data);for(const e in t)this.data[e]=t[e];for(const e in t)e in this.callbackMap&&this.callbackMap[e](this.data[e])}subscribe(t,e){this.callbackMap[t]=e,o()((e=>this.callbackMap[t](this.data[t])))}sendPendingData(){this.jcomm.send_data(this.pendingData),this.pendingData={}}}class c{constructor(t,e,n=\"open\"){this._fire_callback=this._fire_callback.bind(this),this._register=this._register.bind(this),this.jcomm=void 0,this.callback=e,void 0!==window.Jupyter?\"register\"===n?Jupyter.notebook.kernel.comm_manager.register_target(t,this._register):(this.jcomm=Jupyter.notebook.kernel.comm_manager.new_comm(t),this.jcomm.on_msg(this._fire_callback)):void 0!==window._mgr&&(\"register\"===n?window._mgr.widgetManager.proxyKernel.registerCommTarget(t,this._register):(this.jcomm=window._mgr.widgetManager.proxyKernel.createComm(t),this.jcomm.open({},\"\"),this.jcomm.onMsg=this._fire_callback))}send_data(t){void 0!==this.jcomm?this.jcomm.send(t):console.error(\"Jupyter comm module not yet loaded! So we can't send the message.\")}_register(t,e){this.jcomm=t,this.jcomm.on_msg(this._fire_callback)}_fire_callback(t){this.callback(t.content.data)}}class d{constructor(t,n){e(this),this.id=t,this.comm=new s(t),this.comm.subscribe(\"append\",this.appendData),this.comm.subscribe(\"replace\",this.replaceData),this.comm.subscribe(\"event\",this.eventOccurred),this.element=document.getElementById(\"guidance-content-\"+t),this.stop_button=document.getElementById(\"guidance-stop-button-\"+t),this.stop_button.onclick=()=>this.comm.send(\"event\",\"stop\")}appendData(t){t&&(this.stop_button.style.display=\"inline-block\",this.element.innerHTML+=t)}replaceData(t){t&&(this.stop_button.style.display=\"inline-block\",this.element.innerHTML=t)}eventOccurred(t){\"complete\"===t&&(this.stop_button.style.display=\"none\")}}window._guidanceDisplay=function(t,e){return new d(t,e)}})()})();; window._guidanceDisplay(\"94f6e980-59ae-4c42-8971-6fa8bec98498\");</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<system>You are a helpful and terse assistant.\n",
      "\n",
      "</system>\n",
      "\n",
      "<question>What is the name of the system?\n",
      "\n",
      "</question>\n",
      "\n",
      "<answer>The system's name is \"System\".\n",
      "\n",
      "</answer>\n",
      "\n",
      "<question>How can you access the system?\n",
      "\n",
      "</question>\n",
      "\n",
      "<answer>To access the system, use the command \"system\" followed by the desired action (e.g., \"system list\", \"system create\").\n",
      "\n",
      "</answer>\n",
      "\n",
      "<question>What actions can be performed on the system?\n",
      "\n",
      "</question>\n",
      "\n",
      "<answer>Actions that can be performed on the system include creating new systems, listing existing systems, deleting existing systems, and modifying existing systems.\n",
      "\n",
      "</answer>\n",
      "\n",
      "<question>Is there any documentation available for the system?\n",
      "\n",
      "</question>\n",
      "\n",
      "<answer>Yes, there is documentation available for the system. The documentation can be found at <a href=\"https://www.example.com/documentation\">https://www.example.com/documentation</a>.\n",
      "\n",
      "</answer>\n",
      "\n",
      "<question>Are there any known issues with the system?\n",
      "\n",
      "</question>\n",
      "\n",
      "<answer>No, there are no known issues with the system.\n",
      "\n",
      "</answer>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(experts(query='How can I be more productive?', llm=llama))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
